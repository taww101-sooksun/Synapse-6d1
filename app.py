import numpy as np
import streamlit as st
from scipy.io impor‡∏Å‡∏≥‡∏†‡∏πt wavfile
import librosa
import time
import io

# **************************import numpy as np
import torch
import tensorflow as tf
import os

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: RLHF Therapy AI (‡∏™‡∏°‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÇ‡∏ï‡πâ) ---
class TherapyEngine:
    def __init__(self, policy_path=None, llm_path=None):
        self.is_rl_live = False
        self.is_llm_live = False
        # ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .pth ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏™‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        if policy_path and os.path.exists(policy_path):
            self.is_rl_live = True 
        
    def decide_strategy(self, user_text):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå (Empathy, Encouragement, etc.)"""
        # Logic: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå user_text -> ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ RL Model -> ‡πÑ‡∏î‡πâ Strategy + V/A Score
        mood_score = 0.5 # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (Neutral)
        if "‡πÄ‡∏®‡∏£‡πâ‡∏≤" in user_text: mood_score = 0.2
        elif "‡∏î‡∏µ" in user_text: mood_score = 0.8
        
        return {
            "strategy": "Empathy", 
            "valence": mood_score, 
            "arousal": 0.5
        }

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: RBF Music AI (‡∏™‡∏°‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á) ---
class MusicSynthesisEngine:
    def __init__(self, rnn_path=None, vocoder_path=None):
        self.is_rnn_live = False
        self.is_vocoder_live = False
        # ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .h5 ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏™‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        if rnn_path and os.path.exists(rnn_path):
            self.is_rnn_live = True

    @tf.function(experimental_relax_shapes=True)
    def fast_inference(self, symbolic_data):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏î‡πâ‡∏ß‡∏¢ TensorFlow Graph"""
        # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Real AI ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
        return self.rnn_model(symbolic_data)

    def generate_audio(self, valence, arousal, chords):
        """‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏£‡πå‡∏î‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á"""
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Symbolic Sequence
        # 2. ‡∏£‡∏±‡∏ô RNN ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ MFCC
        # 3. ‡∏£‡∏±‡∏ô Vocoder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Audio Wave)
        # 4. ‡∏ó‡∏≥ Mastering (Limiter/Normalize)
        return np.random.uniform(-1, 1, 44100) # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Mock ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
***************************************************
# Note: This code simulates music synthesis due to the absence of TensorFlow/Vocoder.
# The parts requiring external libraries (e.g., sound synthesis from MFCC) 
# are replaced by random audio data generation (Placeholder Audio Generation).
# ******************************************************************************

# -----------------------------------------------------------
# 1. INPUT MODULE (Manages Symbolic Data)
# -----------------------------------------------------------

class InputModule:
    """Manages the conversion of symbolic music data and emotion into a Symbolic Sequence."""
    ROOT_VOCAB = {"C": 0, "C#": 1, "Db": 1, "D": 2, "D#": 3, "Eb": 3, "E": 4, "F": 5, "F#": 6, "Gb": 6, "G": 7, "G#": 8, "Ab": 8, "A": 9, "A#": 10, "Bb": 10, "B": 11} 
    
    def ‡πÅ‡∏õ‡∏•‡∏á_‡∏Ñ‡∏≠‡∏£‡πå‡∏î_‡πÄ‡∏õ‡πá‡∏ô_‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç(self, chord_string):
        """Placeholder: Converts chord name to Chord Index (using a random value instead of actual conversion)"""
        if not chord_string:
             return 0
        try:
            root = chord_string.split()[0].upper()
            return self.ROOT_VOCAB.get(root, 0) # Use Root Note as the basic Index
        except:
             return 0

    def ‡∏à‡∏±‡∏î_‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á_‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á(self, ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ñ‡∏≠‡∏£‡πå‡∏î, valence, arousal):
        """
        Combines Symbolic Data (Chord Index, Valence, Arousal) into a Symbolic Sequence
        (Array A used for data merging)
        """
        # Assume each chord command has a length of 50 time steps, resulting in 10 chords
        num_chords = len(‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ñ‡∏≠‡∏£‡πå‡∏î.split(','))
        total_length = num_chords * 50 if num_chords > 0 else 500
        
        # 3 Features: [Chord Index, Valence, Arousal]
        symbolic_sequence = np.zeros((total_length, 3)) 
        
        chord_indices = [self.‡πÅ‡∏õ‡∏•‡∏á_‡∏Ñ‡∏≠‡∏£‡πå‡∏î_‡πÄ‡∏õ‡πá‡∏ô_‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç(c.strip()) for c in ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ñ‡∏≠‡∏£‡πå‡∏î.split(',') if c.strip()]
        
        if chord_indices:
            # Repeat Chord Index across the length of relevant Time Steps
            for i, index in enumerate(chord_indices):
                start = i * 50
                end = (i + 1) * 50
                symbolic_sequence[start:end, 0] = index # Chord Index
        
        # Assign Valence and Arousal to every Time Step
        symbolic_sequence[:, 1] = valence
        symbolic_sequence[:, 2] = arousal
        
        # Log: Display data structure
        st.sidebar.markdown(f"**Symbolic Sequence (Array A) Generated:** {symbolic_sequence.shape} (Time Steps, Features)")
        
        return symbolic_sequence

# -----------------------------------------------------------
# 2. AI SYNTHESIS ENGINE (Manages RNN and musical details)
# -----------------------------------------------------------

class AISynthesisEngine:
    """Manages RNN processing and musical detail synthesis (Rhythm-Based Features)."""
    def __init__(self, samplerate=44100):
        self.sampling_rate = samplerate
        # self.rnn_model = self.build_RNN_model(...) # Must load a trained model

    def ‡∏à‡∏±‡∏î_‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á_‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•_‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö_RNN(self, merged_data, seq_length=8):
        """Converts 2D data to 3D for LSTM/RNN model (X: Samples, Time Steps, Features)"""
        # This code is skipped in this demonstration as we don't send it to an actual model
        return np.array([[]]), np.array([]) 

    def ‡∏™‡∏£‡πâ‡∏≤‡∏á_Vibrato_Wave(self, amplitude, frequency, duration_sec):
        """Creates a Vibrato wave (Pitch modulation)"""
        time = np.linspace(0, duration_sec, int(self.sampling_rate * duration_sec), endpoint=False)
        return amplitude * np.sin(2 * np.pi * frequency * time)

    def ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå_‡∏î‡πâ‡∏ß‡∏¢_‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î_RBF(self, symbolic_sequence):
        """
        Placeholder: Uses Symbolic Sequence to predict MFCC features (or Mel-spectrogram)
        (In a real scenario, an RNN/Decoder Model would be used here)
        """
        st.sidebar.markdown("---")
        st.sidebar.markdown("**AI Synthesis Engine Processing...**")
        st.sidebar.markdown("1. Preparing Data for RNN...")
        st.sidebar.markdown("2. **RNN/Transformer Inference** (Mock: Generating MFCC features)...")
        
        # Placeholder: Assume the model predicts MFCC features
        # Time steps: Equal to Symbolic Sequence | Features: 40 (Standard MFCC)
        mfcc_features = np.random.rand(symbolic_sequence.shape[0], 40) 

        # Logic: (Vibrato and Pitch Correction / Rhythm Humanization)
        # 1. Calculating Vibrato/Rhythm Humanization (Done in Symbolic/Feature Domain)
        #    - e.g., mfcc_features[:, 5] += self.‡∏™‡∏£‡πâ‡∏≤‡∏á_Vibrato_Wave(...)
        
        st.sidebar.markdown("3. Applying Rhythm Humanization & Vibrato Correction...")
        
        return mfcc_features

# -----------------------------------------------------------
# 3. MASTERING MODULE (Manages Audio Quality)
# -----------------------------------------------------------

class MasteringModule:
    """Manages converting audio features to Raw Audio and audio mastering."""
    def ‡πÉ‡∏ä‡πâ_Limiter(self, ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á, ceiling_value=0.99):
        """Applies a Limiter to cut Peak Value and prevent Clipping"""
        # Adjusts to the range [-1.0, 1.0] for Floating Point Audio
        return np.clip(‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á, -ceiling_value, ceiling_value)

    def ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô_‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏•‡∏á_‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢(self, mfcc_features, samplerate=44100):
        """
        Converts MFCCs back to Raw Audio and performs Mastering 
        (Actual operation requires PyWorld/Vocoder)
        """
        st.sidebar.markdown("---")
        st.sidebar.markdown("**Mastering Module Processing...**")
        st.sidebar.markdown("1. **Vocoder** (Mock: Convert MFCC features back to Raw Audio)...")
        
        # 1. Convert MFCCs back to Raw Audio (Mock: Create 5 seconds of random sound)
        try:
             # Calculate expected duration based on a frame rate (e.g., 50 frames/sec)
            duration_sec = mfcc_features.shape[0] / 50 
        except ZeroDivisionError:
            duration_sec = 5 # Default 5 seconds if array is empty
            
        # Ensure duration is reasonable, otherwise use a default length
        if duration_sec <= 0 or duration_sec > 60:
            duration_sec = 5
            
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå = np.random.uniform(-0.5, 0.5, int(samplerate * duration_sec)) 
        
        # 2. Apply Limiter
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_‡∏à‡∏≥‡∏Å‡∏±‡∏î = self.‡πÉ‡∏ä‡πâ_Limiter(‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)
        st.sidebar.markdown("2. Applying Limiter (Peak Value Clipping)...")
        
        # 3. Adjust LUFS loudness (requires pyloudnorm, simulated)
        # Khomul_siang_Mastered = self.adjust_LUFS_loudness(Khomul_siang_limited, target_lufs=-14.0)
        
        # Simulate loudness adjustment and 16-bit conversion
        # Makes the sound signal slightly louder
        scaling_factor = 0.5
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_Mastered = (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_‡∏à‡∏≥‡∏Å‡∏±‡∏î * scaling_factor * 32767).astype(np.int16)
        st.sidebar.markdown("3. LUFS Normalization (Mock) & Final Bit Depth Conversion (16-bit)...")
        
        return ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á_Mastered, samplerate

# -----------------------------------------------------------
# 4. MAIN APPLICATION LOGIC (Sequence 1 -> 2 -> 3)
# -----------------------------------------------------------

class RBAISystem:
    """The main system that runs all music synthesis steps."""
    def __init__(self):
        self.input_module = InputModule()
        self.ai_engine = AISynthesisEngine()
        self.mastering_module = MasteringModule()

    def ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå_‡πÄ‡∏û‡∏•‡∏á_RBF(self, chord_sequence, emotion_dict):
        # Sequence 1: Input (Symbolic Sequence)
        symbolic_seq = self.input_module.‡∏à‡∏±‡∏î_‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á_‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á(
            chord_sequence, 
            emotion_dict['valence'], 
            emotion_dict['arousal']
        )
        
        # Sequence 2: AI Synthesis (MFCC Features)
        mfcc_features = self.ai_engine.‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå_‡∏î‡πâ‡∏ß‡∏¢_‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î_RBF(symbolic_seq)
        
        # Sequence 3: Mastering and Raw Audio Output
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á, samplerate = self.mastering_module.‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô_‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏•‡∏á_‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢(mfcc_features)
        
        return ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á, samplerate

# -----------------------------------------------------------
# 5. STREAMLIT UI 
# -----------------------------------------------------------

# Web page setup
st.set_page_config(layout="wide", page_title="RBF AI Music Synthesizer (‡∏à‡∏≥‡∏•‡∏≠‡∏á)")
st.title("‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏•‡∏á RBF AI (Rhythm-Based Feature)")
st.subheader("‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI Music Generation Engine")

system = RBAISystem()

with st.expander("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°", expanded=False):
    st.markdown("""
        ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á 3-Stage: **Input** (Symbolic Data) $\\rightarrow$ **AI Synthesis** (RNN/RBF) $\\rightarrow$ **Mastering** (Vocoder/LUFS)
        
        ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (TensorFlow/Vocoder) ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏à‡∏∂‡∏á‡∏ñ‡∏π‡∏Å **‡∏à‡∏≥‡∏•‡∏≠‡∏á** ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå WAV ‡∏™‡∏∏‡πà‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Mastering ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏à‡∏ô‡∏à‡∏ö
    """)

# --- Input Control Section (Symbolic and Emotional Data) ---
st.header("1. Symbolic & Emotional Input")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("##### üéπ Chord Sequence")
    chord_input = st.text_input(
        "‡∏õ‡πâ‡∏≠‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏£‡πå‡∏î (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô Cmaj7, Fm, G7)", 
        "Cmaj7, Am, F, G", 
        key="chord_input"
    )

with col2:
    st.markdown("##### üòå Valence (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç/‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏ö‡∏ß‡∏Å)")
    valence_input = st.slider(
        "‡∏£‡∏∞‡∏î‡∏±‡∏ö Valence (0 = ‡∏•‡∏ö, 1 = ‡∏ö‡∏ß‡∏Å)", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.7, 
        step=0.01
    )

with col3:
    st.markdown("##### ‚ö° Arousal (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô/‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô)")
    arousal_input = st.slider(
        "‡∏£‡∏∞‡∏î‡∏±‡∏ö Arousal (0 = ‡∏™‡∏á‡∏ö, 1 = ‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô)", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.6, 
        step=0.01
    )

emotion_data = {
    'valence': valence_input,
    'arousal': arousal_input
}

st.markdown("---")

# --- Process and Output Control Section ---
if st.button("üöÄ ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏•‡∏á‡∏î‡πâ‡∏ß‡∏¢ RBF AI", type="primary"):
    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå 3 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô..."):
        try:
            # Run the main system
            audio_data_int16, samplerate = system.‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå_‡πÄ‡∏û‡∏•‡∏á_RBF(chord_input, emotion_data)
            
            st.success("‚úÖ ‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
            
            st.header("3. Final Audio Output")
            st.write(f"‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (Sampling Rate: {samplerate} Hz)")
            
            # Display Audio (Must convert Int16 back to Float for st.audio display)
            audio_data_float = audio_data_int16.astype(np.float32) / 32767.0
            st.audio(audio_data_float, format='audio/wav', sample_rate=samplerate)
            
            # Allow downloading the Mastered audio file
            buffer = io.BytesIO()
            wavfile.write(buffer, samplerate, audio_data_int16)
            
            st.download_button(
                label="‚¨áÔ∏è ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå WAV (‡∏à‡∏≥‡∏•‡∏≠‡∏á)",
                data=buffer.getvalue(),
                file_name="final_track_rbf_ai.wav",
                mime="audio/wav"
            )

            # Display processing results
            st.markdown("---")
            st.markdown("### ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡∏î‡∏π‡πÉ‡∏ô Sidebar)")
            st.info("‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Input, AI Engine, ‡πÅ‡∏•‡∏∞ Mastering Module ‡πÉ‡∏ô Sidebar ‡∏ó‡∏≤‡∏á‡∏ã‡πâ‡∏≤‡∏¢")

        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {e}")

else:
    st.info("‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° **‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏•‡∏á‡∏î‡πâ‡∏ß‡∏¢ RBF AI** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£")
    
# --- Sidebar for displaying Processing Log ---
st.sidebar.title("üõ†Ô∏è RBF Engine Log")
st.sidebar.markdown("‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Module")

if st.button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï Log"):
    st.sidebar.info("Log ‡∏ñ‡∏π‡∏Å‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï (‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)")
    pass

from flask import Flask, request, jsonify
from flask_cors import CORS # ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏î‡πÄ‡∏°‡∏ô

app = Flask(__name__)
CORS(app) # ‡πÄ‡∏õ‡∏¥‡∏î CORS

system = RBAISystem()

@app.route('/synthesize', methods=['POST'])
def synthesize_music_api():
    """Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Chord/Emotion ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏•‡∏á"""
    
    # 1. ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input ‡∏à‡∏≤‡∏Å JSON
    try:
        data = request.get_json()
        chord_sequence = data.get('chord_input', 'C, F, G, C')
        valence = data.get('valence', 0.5)
        arousal = data.get('arousal', 0.5)
        
        emotion_data = {
            'valence': float(valence),
            'arousal': float(arousal)
        }
    except Exception as e:
        return jsonify({"error": "Invalid input data: " + str(e)}), 400
        
    # 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    try:
        audio_data_int16, samplerate = system.‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå_‡πÄ‡∏û‡∏•‡∏á_RBF(chord_sequence, emotion_data)
        
        # 3. ‡πÅ‡∏õ‡∏•‡∏á Audio ‡πÄ‡∏õ‡πá‡∏ô Base64 String ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏≤‡∏á API
        import base64
        import io
        from scipy.io import wavfile
        
        wav_io = io.BytesIO()
        wavfile.write(wav_io, samplerate, audio_data_int16)
        wav_bytes = wav_io.getvalue()
        audio_base64 = base64.b64encode(wav_bytes).decode('utf-8')
        
        # 4. ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON
        return jsonify({
            "status": "success",
            "message": "Music synthesis complete. Ready for emotional resonance.",
            "audio_base64": audio_base64,
            "samplerate": samplerate,
            "input": {
                "chords": chord_sequence,
                "valence": valence,
                "arousal": arousal
            }
        })
        
    except Exception as e:
        # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà AI Model ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ Vocoder ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        return jsonify({"error": f"Synthesis Error: {e}"}), 500

if __name__ == '__main__':
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÅ‡∏ï‡πà‡πÉ‡∏ô Cloud Functions ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Deploy ‡∏à‡∏£‡∏¥‡∏á
    # app.run(debug=True)
    pass
